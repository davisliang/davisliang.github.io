---
layout: page
permalink: /XLM-V/
title: XLM-V
nav: true
--- 

XLM-V is a multilingual masked language model based on the XLM-R (XLM-RoBERTa) architecture with a 1M token vocabulary. It is trained on 2.5T of filtered CommonCrawl data in 100 languages. XLM-V outperforms XLM-R on every multilingual task we tested it on (XNLI, MLQA, TyDiQA, XQuAD, WikiAnn) with outsized gains on low-resource language tasks (MasakhaNER, AmericasNLI). [[Paper]](https://arxiv.org/abs/2301.10472) [[Download]](https://dl.fbaipublicfiles.com/fairseq/xlmv/xlmv.base.tar.gz) [[Instructions]](https://github.com/davisliang/fairseq/tree/main/examples/xlmv)
